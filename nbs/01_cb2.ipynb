{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| default_exp cb2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_or_compute_embeddings(compute_embeddings):\n",
    "    \"\"\"\n",
    "    Checks for required precomputed files and either loads them or computes and saves them.\n",
    "    \n",
    "    Parameters:\n",
    "      compute_embeddings (callable): A function that computes and returns the embeddings.\n",
    "    \n",
    "    Returns:\n",
    "      dict: The loaded or newly computed embeddings.\n",
    "    \"\"\"\n",
    "    required_files = [\n",
    "        \"graph_chunk_entity_relation.graphml\",\n",
    "        \"kv_store_text_chunks.json\",\n",
    "        \"kv_store_doc_status.json\",\n",
    "        \"vdb_chunks.json\",\n",
    "        \"kv_store_full_docs.json\",\n",
    "        \"vdb_entities.json\",\n",
    "        \"kv_store_llm_response_cache.json\",\n",
    "        \"vdb_relationships.json\"\n",
    "    ]\n",
    "    \n",
    "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print(\"Missing files detected:\", missing_files)\n",
    "        # Compute embeddings using the provided function\n",
    "        embeddings = compute_embeddings()\n",
    "        # Save results to one or more of the required files\n",
    "        # (You may need to adapt this to your file structure)\n",
    "        with open(\"vdb_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(embeddings, f)\n",
    "        # Save other files as needed...\n",
    "    else:\n",
    "        print(\"All precomputed files found. Loading cached embeddings...\")\n",
    "        with open(\"vdb_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            embeddings = json.load(f)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LightRAG' from 'lightrag' (/opt/homebrew/Caskroom/miniforge/base/envs/lightrag/lib/python3.10/site-packages/lightrag/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightrag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightRAG, QueryParam\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ollama_model_complete, ollama_embed\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmbeddingFunc\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LightRAG' from 'lightrag' (/opt/homebrew/Caskroom/miniforge/base/envs/lightrag/lib/python3.10/site-packages/lightrag/__init__.py)"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#| label: rag-query-system\n",
    "#| fig-cap: RAG Query Interface\n",
    "#| description: Cached query system that prevents redundant executions\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "import inspect\n",
    "import logging\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "WORKING_DIR = \"./dickens\"\n",
    "SENTINEL_FILE = Path(\".rag_initialized\")\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "def _ensure_workspace():\n",
    "    \"\"\"Create workspace directory if missing\"\"\"\n",
    "    os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "async def initialize_rag():\n",
    "    \"\"\"Initialize RAG system with caching check\"\"\"\n",
    "    if SENTINEL_FILE.exists():\n",
    "        logging.info(\"RAG system already initialized - using cached instance\")\n",
    "        return LightRAG.from_cache(WORKING_DIR)\n",
    "    \n",
    "    logging.info(\"Initializing fresh RAG instance\")\n",
    "    rag = LightRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        llm_model_func=ollama_model_complete,\n",
    "        llm_model_name=\"deepseek-r1:1.5b1a\",\n",
    "        llm_model_max_async=4,\n",
    "        llm_model_max_token_size=32768,\n",
    "        llm_model_kwargs={\n",
    "            \"host\": \"http://localhost:11434\",\n",
    "            \"options\": {\"num_ctx\": 32768},\n",
    "        },\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=768,\n",
    "            max_token_size=8192,\n",
    "            func=lambda texts: ollama_embed(\n",
    "                texts, embed_model=\"nomic-embed-text\", host=\"http://localhost:11434\"\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "    SENTINEL_FILE.touch()\n",
    "    return rag\n",
    "\n",
    "async def print_stream(stream):\n",
    "    \"\"\"Handle streaming responses\"\"\"\n",
    "    async for chunk in stream:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "def _load_data():\n",
    "    \"\"\"Load CSV data with existence check\"\"\"\n",
    "    csv_path = Path(\"./42a.csv\")\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"CSV file not found at {csv_path}\")\n",
    "    \n",
    "    with csv_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return \"\\n\".join(\n",
    "            f\"Topic: {row['Topic']}\\nKey Concepts/Themes: {row['Key Concepts/Themes']}\"\n",
    "            for row in csv.DictReader(f)\n",
    "        )\n",
    "\n",
    "def execute_queries(rag):\n",
    "    \"\"\"Execute all predefined queries\"\"\"\n",
    "    queries = {\n",
    "        \"Naive Search\": QueryParam(mode=\"naive\"),\n",
    "        \"Local Search\": QueryParam(mode=\"local\"),\n",
    "        \"Global Search\": QueryParam(mode=\"global\"),\n",
    "        \"Hybrid Search\": QueryParam(mode=\"hybrid\")\n",
    "    }\n",
    "    \n",
    "    for name, param in queries.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        result = rag.query(\n",
    "            \"Can you summarize how each topic combines concepts of self-expression, beauty, and resource management?\",\n",
    "            param=param\n",
    "        )\n",
    "        print(result if not inspect.isasyncgen(result) else asyncio.run(print_stream(result)))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution flow with cache check\"\"\"\n",
    "    _ensure_workspace()\n",
    "    \n",
    "    if SENTINEL_FILE.exists() and not os.getenv(\"FORCE_RERUN\"):\n",
    "        logging.info(\"Skipping execution - use FORCE_RERUN=1 to override\")\n",
    "        return\n",
    "    \n",
    "    rag = asyncio.run(initialize_rag())\n",
    "    rag.insert(_load_data())\n",
    "    execute_queries(rag)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
